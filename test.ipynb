{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools/impl.py\n",
    "from __future__ import annotations\n",
    "import os, json, requests, pymysql\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# ───────────────────────────────────────────\n",
    "# 1) PubMed\n",
    "# ───────────────────────────────────────────\n",
    "PUBMED = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "\n",
    "def pubmed_search(term: str, retmax: int = 10) -> List[str]:\n",
    "    params = {\"db\": \"pubmed\", \"term\": term, \"retmode\": \"json\",\n",
    "              \"sort\": \"pub+date\", \"retmax\": retmax}\n",
    "    r = requests.get(f\"{PUBMED}/esearch.fcgi\", params=params, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"esearchresult\"][\"idlist\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80264d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ret_cacfe0e74802',\n",
       " 'question': 'What is the PMID of the article titled “[Practical guideline for short bowel syndrome]” by first author Dabsch S, published in Zeitschrift fur Gastroenterologie in 2025?',\n",
       " 'tool_calls': {'tool': ['pubmed.search'],\n",
       "  'params': ['{\"term\":\"\\\\\"[Practical guideline for short bowel syndrome].\\\\\"[ti] AND Dabsch S[au] AND 2025[dp]\",\"retmax\":1}']},\n",
       " 'answer': ['40360142'],\n",
       " 'type': 'retrieval'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"DiligentDing/MAIA\", split=\"full\")  # loads the entire benchmark\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CTGOV = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "\n",
    "def ctgov_search(filter_expr: str, page_size: int = 100) -> List[str]:\n",
    "    r = requests.get(CTGOV, params={\"filter\": filter_expr,\n",
    "                                    \"pageSize\": page_size}, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    rows = r.json().get(\"studies\", [])\n",
    "    return [s[\"protocolSection\"][\"identificationModule\"][\"nctId\"] for s in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "OT_URL = \"https://api.platform.opentargets.org/api/v4/graphql\"\n",
    "def _ot_query(query: str) -> Dict:\n",
    "    r = requests.post(OT_URL, json={\"query\": query}, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"data\"]\n",
    "\n",
    "def ot_associated_diseases(target_id: str, min_score: float = 0.5) -> List[Dict]:\n",
    "    q = f\"\"\"\n",
    "    {{ target(ensemblId: \"{target_id}\") {{\n",
    "        associatedDiseases {{ rows {{ disease {{id name}} score }} }} }} }}\n",
    "    \"\"\"\n",
    "    rows = _ot_query(q)[\"target\"][\"associatedDiseases\"][\"rows\"]\n",
    "    return [r for r in rows if r[\"score\"] >= min_score]\n",
    "def ot_tractability(target_id: str, value: bool = True) -> List[Dict]:\n",
    "    q = f\"\"\"\n",
    "    {{ target(ensemblId: \"{target_id}\") {{\n",
    "        tractability {{ modality label value }} }} }}\n",
    "    \"\"\"\n",
    "    rows = _ot_query(q)[\"target\"][\"tractability\"]\n",
    "    return [r for r in rows if r[\"value\"] is value]\n",
    "\n",
    "\n",
    "def ot_safety(symbol: str, event: str) -> Dict:\n",
    "    # 1. 查找target ID\n",
    "    search_q = f'''\n",
    "    {{\n",
    "      search(queryString: \"{symbol}\") {{\n",
    "        hits {{\n",
    "          id\n",
    "          entity\n",
    "          description\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    '''\n",
    "    search_data = _ot_query(search_q)[\"search\"][\"hits\"]\n",
    "    # 过滤出entity为target的\n",
    "    target_id = None\n",
    "    for hit in search_data:\n",
    "        if hit[\"entity\"] == \"target\":\n",
    "            target_id = hit[\"id\"]\n",
    "            break\n",
    "    if not target_id:\n",
    "        return {}\n",
    "    \n",
    "    # 2. 用ID查safetyLiabilities\n",
    "    safety_q = f'''\n",
    "    {{\n",
    "      target(ensemblId: \"{target_id}\") {{\n",
    "        safetyLiabilities {{\n",
    "          event\n",
    "          biosamples {{ tissueLabel tissueId }}\n",
    "          effects {{ dosing direction }}\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    '''\n",
    "    rows = _ot_query(safety_q)[\"target\"][\"safetyLiabilities\"]\n",
    "    for r in rows:\n",
    "        if r[\"event\"].lower() == event.lower():\n",
    "            return {\"biosamples\": r[\"biosamples\"], \"effects\": r[\"effects\"]}\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257526af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────\n",
    "DB_CFG = dict(\n",
    "    host=\"172.188.121.85\",\n",
    "    port=3306,\n",
    "    user=\"root\",\n",
    "    password=\"1qaz0plm\",\n",
    "    database=\"umls\",\n",
    "    cursorclass=pymysql.cursors.DictCursor,\n",
    "    autocommit=True,\n",
    ")\n",
    "_conn = pymysql.connect(**DB_CFG)\n",
    "\n",
    "def umls_concept_lookup(name: str) -> str:\n",
    "    with _conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT cui FROM MRCONSO WHERE STR = %s LIMIT 1\", (name,))\n",
    "        row = cur.fetchone()\n",
    "        return row[\"cui\"] if row else \"\"\n",
    "\n",
    "def umls_get_related(from_cui: str, rela: str) -> List[str]:\n",
    "    with _conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"SELECT cui1 FROM MRREL WHERE cui2=%s AND rela=%s\",\n",
    "            (from_cui, rela))\n",
    "        return [row[\"cui1\"] for row in cur.fetchall()]\n",
    "def umls_cui_to_name(cui: str) -> str:\n",
    "    \"\"\"\n",
    "    输入CUI，返回英文名，优先PF/PT类型，否则任选一个。\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT STR, TTY\n",
    "        FROM   MRCONSO\n",
    "        WHERE  LAT='ENG' AND CUI=%s\n",
    "    \"\"\"\n",
    "    with _conn.cursor() as cur:\n",
    "        cur.execute(sql, (cui,))\n",
    "        names = []\n",
    "        pfpt = None\n",
    "        for row in cur.fetchall():\n",
    "            names.append(row[\"STR\"])\n",
    "            if row[\"TTY\"] in (\"PF\", \"PT\"):\n",
    "                pfpt = row[\"STR\"]\n",
    "        if pfpt:\n",
    "            return pfpt\n",
    "        if names:\n",
    "            return names[0]\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce500513",
   "metadata": {},
   "outputs": [],
   "source": [
    "umls_cui_to_name(\"C1521863\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict, List\n",
    "\n",
    "CTGOV_API_URL = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "\n",
    "def parse_filter_expr(filter_expr: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    解析形如 key1=val1;key2=val2 的表达式为 dict\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for seg in filter_expr.split(\";\"):\n",
    "        if \"=\" in seg:\n",
    "            k, v = seg.split(\"=\", 1)\n",
    "            res[k.strip()] = v.strip()\n",
    "    return res\n",
    "\n",
    "import requests\n",
    "\n",
    "def ctgov_search(filter_expr: str, page_size: int = 100):\n",
    "    filters = {}\n",
    "    interventions = []\n",
    "    locations = {}\n",
    "    # 解析 filter_expr\n",
    "    for seg in filter_expr.split(\";\"):\n",
    "        if \"=\" in seg:\n",
    "            k, v = seg.split(\"=\", 1)\n",
    "            k, v = k.strip(), v.strip()\n",
    "            if k == \"interventions.name\":\n",
    "                interventions.append(v)\n",
    "            elif k == \"locations.country\":\n",
    "                locations[\"country\"] = v\n",
    "            elif k == \"startDateFrom\":\n",
    "                filters.setdefault(\"startDate\", {})[\"from\"] = v\n",
    "            else:\n",
    "                filters[k] = v\n",
    "\n",
    "    if interventions:\n",
    "        filters.setdefault(\"interventions\", {})[\"name\"] = interventions\n",
    "    if locations:\n",
    "        filters[\"locations\"] = locations\n",
    "\n",
    "    payload = {\n",
    "        \"filters\": filters,\n",
    "        \"pageSize\": page_size\n",
    "    }\n",
    "\n",
    "    r = requests.post(\n",
    "        \"https://clinicaltrials.gov/api/v2/studies\",\n",
    "        json=payload,\n",
    "        timeout=20\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"studies\", [])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctgov_search_fixed.py\n",
    "import requests, urllib.parse\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "BASE_URL = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "\n",
    "def _build_params(expr: str,\n",
    "                  page_size: int,\n",
    "                  page_token: Optional[str]) -> Dict[str, str]:\n",
    "    params: Dict[str, str] = {\n",
    "        \"pageSize\": str(page_size),\n",
    "        \"countTotal\": \"false\",\n",
    "        \"markupFormat\": \"markdown\",\n",
    "    }\n",
    "    interventions, advanced = [], []\n",
    "\n",
    "    for seg in filter(None, (s.strip() for s in expr.split(\";\"))):\n",
    "        k, v = map(str.strip, seg.split(\"=\", 1))\n",
    "\n",
    "        if k == \"conditions\":\n",
    "            params[\"query.cond\"] = v\n",
    "\n",
    "        elif k == \"interventions.name\":\n",
    "            interventions.append(v)\n",
    "\n",
    "        elif k == \"locations.country\":\n",
    "            params[\"query.locn\"] = f'\"{v}\"'          # 加引号避免拆词\n",
    "\n",
    "        elif k == \"overallStatus\":\n",
    "            params[\"filter.overallStatus\"] = v.upper()\n",
    "\n",
    "        elif k == \"studyType\":\n",
    "            advanced.append(f\"AREA[StudyType]({v.upper()})\")\n",
    "\n",
    "        elif k == \"startDateFrom\":\n",
    "            advanced.append(f\"AREA[StartDate]RANGE[{v},MAX]\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported key: {k}\")\n",
    "\n",
    "    if interventions:\n",
    "        params[\"query.intr\"] = \" AND \".join(interventions)\n",
    "    if advanced:\n",
    "        params[\"filter.advanced\"] = \" AND \".join(advanced)\n",
    "    if page_token:\n",
    "        params[\"pageToken\"] = page_token\n",
    "    return params\n",
    "\n",
    "\n",
    "def ctgov_search(expr: str, page_size: int = 100) -> List[Dict]:\n",
    "    studies, token = [], None\n",
    "    while True:\n",
    "        params = _build_params(expr, page_size, token)\n",
    "        r = requests.get(BASE_URL, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            # 打印可读 URL 与返回文本帮助调试\n",
    "            print(\"ERROR  :\", r.status_code, r.reason)\n",
    "            print(\"URL    :\", urllib.parse.unquote(r.url))\n",
    "            print(\"BODY   :\", r.text[:500], \"...\")\n",
    "            r.raise_for_status()\n",
    "\n",
    "        data = r.json()\n",
    "        studies.extend(data.get(\"studies\", []))\n",
    "        token = data.get(\"nextPageToken\")\n",
    "        if not token:\n",
    "            break\n",
    "    return studies\n",
    "\n",
    "\n",
    "# ------------------- quick demo -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    fx = (\n",
    "        \"overallStatus=COMPLETED;\"\n",
    "        \"studyType=INTERVENTIONAL;\"\n",
    "        \"conditions=Multiple Myeloma;\"\n",
    "        \"interventions.name=CC-5013;\"\n",
    "        \"interventions.name=Dexamethasone;\"\n",
    "        \"startDateFrom=2003-01-01;\"\n",
    "        \"locations.country=United States\"\n",
    "    )\n",
    "    lst = ctgov_search(fx)\n",
    "    print(\"Fetched:\", len(lst))\n",
    "    for s in lst[:5]:\n",
    "        ident = s[\"protocolSection\"][\"identificationModule\"]\n",
    "        print(ident[\"nctId\"], \"-\", ident[\"briefTitle\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, urllib.parse\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "CTGOV = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "\n",
    "def _build_params(expr: str,\n",
    "                  page_size: int = 100,\n",
    "                  page_token: Optional[str] = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    将形如 'key=value;key=value' 的过滤串转为 v2 API 参数。\n",
    "    支持的 key:\n",
    "      overallStatus, studyType, conditions, interventions.name,\n",
    "      locations.country, startDateFrom\n",
    "    \"\"\"\n",
    "    params: Dict[str, str] = {\n",
    "        \"pageSize\": str(page_size),\n",
    "        \"countTotal\": \"false\",\n",
    "        \"markupFormat\": \"markdown\",\n",
    "    }\n",
    "    interventions, advanced = [], []\n",
    "\n",
    "    for seg in filter(None, (s.strip() for s in expr.split(\";\"))):\n",
    "        k, v = map(str.strip, seg.split(\"=\", 1))\n",
    "\n",
    "        if k == \"conditions\":\n",
    "            params[\"query.cond\"] = v\n",
    "\n",
    "        elif k == \"interventions.name\":\n",
    "            interventions.append(v)\n",
    "\n",
    "        elif k == \"locations.country\":\n",
    "            params[\"query.locn\"] = f'\"{v}\"'    # 用引号避免拆词\n",
    "\n",
    "        elif k == \"overallStatus\":\n",
    "            params[\"filter.overallStatus\"] = v.upper()\n",
    "\n",
    "        elif k == \"studyType\":\n",
    "            advanced.append(f\"AREA[StudyType]({v.upper()})\")\n",
    "\n",
    "        elif k == \"startDateFrom\":\n",
    "            advanced.append(f\"AREA[StartDate]RANGE[{v},MAX]\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported key: {k}\")\n",
    "\n",
    "    if interventions:\n",
    "        params[\"query.intr\"] = \" AND \".join(interventions)\n",
    "    if advanced:\n",
    "        params[\"filter.advanced\"] = \" AND \".join(advanced)\n",
    "    if page_token:\n",
    "        params[\"pageToken\"] = page_token\n",
    "    return params\n",
    "\n",
    "\n",
    "def ctgov_search(filter_expr: str, page_size: int = 100) -> List[str]:\n",
    "    \"\"\"返回符合条件的 NCT ID 列表\"\"\"\n",
    "    studies, token = [], None\n",
    "    while True:\n",
    "        params = _build_params(filter_expr, page_size, token)\n",
    "        r = requests.get(CTGOV, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        data = r.json()\n",
    "        studies.extend(data.get(\"studies\", []))\n",
    "        token = data.get(\"nextPageToken\")\n",
    "        if not token:\n",
    "            break\n",
    "\n",
    "    # 提取 NCT ID\n",
    "    return [\n",
    "        s[\"protocolSection\"][\"identificationModule\"][\"nctId\"]\n",
    "        for s in studies\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_agent.py\n",
    "\"\"\"\n",
    "示例：把所有 schema 注册给 Azure OpenAI，\n",
    "收到 tool_call 请求后执行 tools.impl 中的真实函数，再回传。\n",
    "\"\"\"\n",
    "import os\n",
    "import json, importlib\n",
    "from openai import AzureOpenAI\n",
    "from tools.schema import ALL_SCHEMAS\n",
    "from tools import impl      as T  # 引入实现\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "import re\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-10Dnh4QEV9v2TJQMI8VA4XHSZgB0J0DzfCInl4holuLlNjpH' \n",
    "os.environ['OPENAI_BASE_URL'] = 'https://api2.aigcbest.top/v1' \n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"5a1437f6ff2648b9b969507fb5a73276\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ai-mistraleastus2753718354821.openai.azure.com/\"\n",
    "os.environ[\"YOUR_DEPLOYMENT\"] = \"gpt-4.1-noah\"  # 替换为你的部署名\n",
    "# client = AzureOpenAI(\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "#     api_version=\"2024-12-01-preview\"\n",
    "# )\n",
    "\n",
    "# 映射工具名 → Python 函数\n",
    "# TOOLS = {\n",
    "#     \"pubmed.search\":        T.pubmed_search,\n",
    "#     \"ctgov_search\":         T.ctgov_search,\n",
    "#     \"opentargets.search\":   T.ot_associated_diseases,\n",
    "#     \"opentargets.tractability\": T.ot_tractability,\n",
    "#     \"opentargets.safety\":   T.ot_safety,\n",
    "#     \"umls.concept_lookup\":  T.umls_concept_lookup,\n",
    "#     \"umls.get_related\":     T.umls_get_related,\n",
    "#     \"oncology.path_query\":  T.oncology_path_query,\n",
    "# }\n",
    "TOOLS = {\n",
    "    \"pubmed_search\":            T.pubmed_search,\n",
    "    \"ctgov_search\":             T.ctgov_search,\n",
    "    \"opentargets_search\":       T.ot_associated_diseases,\n",
    "    \"opentargets_tractability\": T.ot_tractability,\n",
    "    \"opentargets_safety\":       T.ot_safety,\n",
    "    \"umls_concept_lookup\":      T.umls_concept_lookup,\n",
    "    \"umls_get_related\":         T.umls_get_related,\n",
    "    \"oncology_path_query\":      T.oncology_path_query,\n",
    "}\n",
    "# demo_agent.py  顶部 import 之后加\n",
    "from copy import deepcopy\n",
    "def make_safe_schemas(schemas):\n",
    "    safe = []\n",
    "    for sch in schemas:\n",
    "        fn = deepcopy(sch)\n",
    "        fn[\"name\"] = fn[\"name\"].replace(\".\", \"_\")\n",
    "        assert re.match(r\"^[a-zA-Z0-9_-]+$\", fn[\"name\"])\n",
    "        safe.append({\"type\": \"function\", \"function\": fn})\n",
    "    return safe\n",
    "\n",
    "def ensure_function_wrapping(schemas: list[dict]):\n",
    "    \"\"\"确保每个 schema 都包含 {\"type\":\"function\", \"function\": ...} 结构\"\"\"\n",
    "    new_list = []\n",
    "    for sch in schemas:\n",
    "        if \"type\" in sch:              # 已是新格式\n",
    "            new_list.append(sch)\n",
    "        else:                          # 旧格式 → 包装\n",
    "            new_list.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": deepcopy(sch)\n",
    "            })\n",
    "    return new_list\n",
    "\n",
    "\n",
    "RAW_SCHEMAS = ALL_SCHEMAS \n",
    "SAFE_SCHEMAS = make_safe_schemas(RAW_SCHEMAS)          # 名字变下划线\n",
    "ALL_SCHEMAS   = ensure_function_wrapping(SAFE_SCHEMAS) # ←覆盖全局变量\n",
    "\n",
    "\n",
    "def chat_once(\n",
    "        user_text: str,\n",
    "        model_name: Optional[str] = None      # ← 替换掉  str | None\n",
    "    ) -> str:\n",
    "\n",
    "    if model_name is None:\n",
    "        model_name = os.getenv(\"YOUR_DEPLOYMENT\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": user_text}\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        # client = OpenAI(\n",
    "        #     api_key=\"sk-5157bc95a7ee4f7e9086a80fd41c69fc\",\n",
    "        #     base_url=\"https://api.deepseek.com/v1\"\n",
    "        # )\n",
    "        client = OpenAI()\n",
    "        rsp = client.chat.completions.create(\n",
    "            model=model_name,          # ← 动态部署名\n",
    "            messages=messages,\n",
    "            tools=ALL_SCHEMAS,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        msg = rsp.choices[0].message\n",
    "        if msg.tool_calls:                 # 有工具调用：执行并回传\n",
    "            for tc in msg.tool_calls:\n",
    "                fn   = TOOLS[tc.function.name]\n",
    "                args = json.loads(tc.function.arguments)\n",
    "                result = fn(**args)\n",
    "                messages.append(msg)\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tc.id,\n",
    "                    \"name\": tc.function.name,\n",
    "                    \"content\": json.dumps(result, ensure_ascii=False)\n",
    "                })\n",
    "            continue                       # 再让模型整合结果\n",
    "        else:\n",
    "            return msg.content.strip()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(chat_once(\"Which PMIDs match “A rare case of rectal malignant melanoma with long-term survival ...”?\",\"deepseek-ai/DeepSeek-R1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a0a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_retrieval.py\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "# from demo_agent import chat_once\n",
    "\n",
    "# ---------- 配置 ----------\n",
    "IN_FILE   = Path(\"MAIA/MAIA_retrieval.json\")\n",
    "OUT_FILE  = Path(\"retrieval_answers.json\")\n",
    "MODEL     = \"deepseek-r1\"          # ← 替换为真实部署名\n",
    "BATCH_SIZE = 10                    # 每 10 条写盘一次\n",
    "\n",
    "# ---------- 读数据 ----------\n",
    "items = json.loads(IN_FILE.read_text(encoding=\"utf-8\"))\n",
    "items = items[\"dataset\"] if isinstance(items, dict) else items\n",
    "\n",
    "# ---------- 读取已完成答案 ----------\n",
    "answered: dict[str, str] = {}\n",
    "if OUT_FILE.exists():\n",
    "    answered = json.loads(OUT_FILE.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "pending = {}   # 本轮新增，达到 BATCH_SIZE 时写盘\n",
    "\n",
    "# ---------- 主循环 ----------\n",
    "for idx, item in enumerate(tqdm(items, desc=\"QA\")):\n",
    "    key = str(idx)\n",
    "    if key in answered:            # 已有 → 跳过\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        reply = chat_once(item[\"question\"], MODEL)\n",
    "    except Exception as e:\n",
    "        reply = f\"[ERROR] {e!s}\"\n",
    "\n",
    "    answered[key] = reply\n",
    "    pending[key]  = reply\n",
    "\n",
    "    # —— 批量落盘 ——\n",
    "    if len(pending) >= BATCH_SIZE:\n",
    "        OUT_FILE.write_text(json.dumps(answered, ensure_ascii=False, indent=2))\n",
    "        pending.clear()\n",
    "\n",
    "# ---------- 收尾：保存剩余未写出的 ----------\n",
    "if pending:\n",
    "    OUT_FILE.write_text(json.dumps(answered, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(f\"✅  共 {len(answered)} answers saved to {OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_agent.py\n",
    "\"\"\"\n",
    "示例：把所有 schema 注册给 Azure OpenAI，\n",
    "收到 tool_call 请求后执行 tools.impl 中的真实函数，再回传。\n",
    "\"\"\"\n",
    "import os\n",
    "import json, importlib\n",
    "from openai import AzureOpenAI\n",
    "from tools.schema import ALL_SCHEMAS\n",
    "from tools import impl      as T  # 引入实现\n",
    "from typing import Optional\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"5a1437f6ff2648b9b969507fb5a73276\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ai-mistraleastus2753718354821.openai.azure.com/\"\n",
    "os.environ[\"YOUR_DEPLOYMENT\"] = \"gpt-4.1-noah\"  # 替换为你的部署名\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "# 映射工具名 → Python 函数\n",
    "TOOLS = {\n",
    "    \"pubmed.search\":        T.pubmed_search,\n",
    "    \"ctgov_search\":         T.ctgov_search,\n",
    "    \"opentargets.search\":   T.ot_associated_diseases,\n",
    "    \"opentargets.tractability\": T.ot_tractability,\n",
    "    \"opentargets.safety\":   T.ot_safety,\n",
    "    \"umls.concept_lookup\":  T.umls_concept_lookup,\n",
    "    \"umls.get_related\":     T.umls_get_related,\n",
    "    \"oncology.path_query\":  T.oncology_path_query,\n",
    "}\n",
    "\n",
    "# demo_agent.py  顶部 import 之后加\n",
    "from copy import deepcopy\n",
    "\n",
    "def ensure_function_wrapping(schemas: list[dict]):\n",
    "    \"\"\"确保每个 schema 都包含 {\"type\":\"function\", \"function\": ...} 结构\"\"\"\n",
    "    new_list = []\n",
    "    for sch in schemas:\n",
    "        if \"type\" in sch:              # 已是新格式\n",
    "            new_list.append(sch)\n",
    "        else:                          # 旧格式 → 包装\n",
    "            new_list.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": deepcopy(sch)\n",
    "            })\n",
    "    return new_list\n",
    "\n",
    "ALL_SCHEMAS = ensure_function_wrapping(ALL_SCHEMAS)   # ← 覆盖原变量\n",
    "\n",
    "def chat_once(\n",
    "        user_text: str,\n",
    "        model_name: Optional[str] = None      # ← 替换掉  str | None\n",
    "    ) -> str:\n",
    "\n",
    "    if model_name is None:\n",
    "        model_name = os.getenv(\"YOUR_DEPLOYMENT\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": user_text}\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        rsp = client.chat.completions.create(\n",
    "            model=model_name,          # ← 动态部署名\n",
    "            messages=messages,\n",
    "            tools=ALL_SCHEMAS,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        msg = rsp.choices[0].message\n",
    "        if msg.tool_calls:                 # 有工具调用：执行并回传\n",
    "            for tc in msg.tool_calls:\n",
    "                fn   = TOOLS[tc.function.name]\n",
    "                args = json.loads(tc.function.arguments)\n",
    "                result = fn(**args)\n",
    "                messages.append(msg)\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tc.id,\n",
    "                    \"name\": tc.function.name,\n",
    "                    \"content\": json.dumps(result, ensure_ascii=False)\n",
    "                })\n",
    "            continue                       # 再让模型整合结果\n",
    "        else:\n",
    "            return msg.content.strip()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(chat_once(\"In ClinicalTrials.gov, what interventional studies for relapsing-remitting multiple sclerosis involving Cladribine 5.25 mg/kg were started in April 2005?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c72248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import difflib\n",
    "import copy\n",
    "import os\n",
    "import pymysql\n",
    "import torch\n",
    "from multiprocessing import Pool\n",
    "import networkx as nx\n",
    "from community import community_louvain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b223c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_analysis(network_json):\n",
    "    start = time.time()\n",
    "    graph = nx.Graph()\n",
    "    for e in network_json['edges']:\n",
    "        graph.add_edge(e['source'], e['target'])\n",
    "    partition = community_louvain.best_partition(graph)\n",
    "    degree_centrality = nx.degree_centrality(graph)\n",
    "    degree = nx.degree(graph)\n",
    "    clustering = nx.clustering(graph)\n",
    "    betweenness = nx.betweenness_centrality(graph)\n",
    "    # average_clustering = nx.average_clustering(graph)\n",
    "    return {'partition': partition,\n",
    "            'degree_centrality': degree_centrality,\n",
    "            'degree': degree,\n",
    "            'clustering': clustering,\n",
    "            'betweenness': betweenness, }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede89173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "a=list(map(int,input().split()))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f2999b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "mat=[list(map(int,input().split())) for _ in range(2)]\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc225b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaad\n"
     ]
    }
   ],
   "source": [
    "s=input().strip()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c415de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "def quicksort(nums):\n",
    "    if len(nums)<=1:\n",
    "        return nums\n",
    "    pivot=nums[len(nums)//2]\n",
    "    left=[x for x in nums if x<pivot]\n",
    "    middle=[x for x in nums if x==pivot]\n",
    "    right=[x for x in nums if x>pivot]\n",
    "    return quicksort(left)+middle+quicksort(right)\n",
    "nums=[3, 6, 8, 10, 1, 2, 1]\n",
    "print(quicksort(nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ad1fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2360681944690217\n"
     ]
    }
   ],
   "source": [
    "x=5\n",
    "y=x/2\n",
    "a=0.001\n",
    "while abs(y*y-x)>1e-6:\n",
    "    grad=4*y*(y*y-x)\n",
    "    y=y-a*grad\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a2a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from functools import cache\n",
    "def minfunc(s,t):\n",
    "    m=len(s)\n",
    "    n=len(t)\n",
    "    @cache\n",
    "    def dfs(i,j):\n",
    "        if i<0:\n",
    "            return j+1\n",
    "        if j<0:\n",
    "            return i+1\n",
    "        if s[i]==t[j]:\n",
    "            return dfs(i-1,j-1)\n",
    "        return min(dfs(i-1,j-1),dfs(i-1,j),dfs(i,j-1))+1\n",
    "    return dfs(m-1,n-1)\n",
    "word1=input().strip()\n",
    "word2=input().strip()\n",
    "res=minfunc(word1,word2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65ee9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def maxprofit(nums):\n",
    "    min_price=nums[0]\n",
    "    max_profit=0\n",
    "    for x in nums:\n",
    "        profit=x-min_price\n",
    "        max_profit=max(max_profit,profit)\n",
    "        min_price=min(min_price,x)\n",
    "    return max_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1784a504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "nums=list(map(int,input().split()))\n",
    "print(maxprofit(nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f1aac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxnum(nums):\n",
    "    min_s=0\n",
    "    ans=nums[0]\n",
    "    s=0\n",
    "    for x in nums:\n",
    "        s+=x\n",
    "        ans=max(ans,s-min_s)\n",
    "        min_s=min(s,min_s)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e20a3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxnum([-2,1,-3,4,-1,2,1,-5,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "449f736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestPalindrome(s):\n",
    "    n=len(s)\n",
    "    if n<2:\n",
    "        return s\n",
    "    dp=[[False]*n for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dp[i][i]=True\n",
    "    start=0\n",
    "    max_len=1\n",
    "    for j in range(1,n):\n",
    "        for i in range(j):\n",
    "            if s[i]==s[j]:\n",
    "                if j-i+1<=2:\n",
    "                    dp[i][j]=True\n",
    "                else:\n",
    "                    dp[i][j]=dp[i+1][j-1]\n",
    "            else:\n",
    "                dp[i][j]=False\n",
    "            if dp[i][j] and (j-i+1>max_len):\n",
    "                max_len=j-i+1\n",
    "                start=i\n",
    "    return s[start:start+max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6adcae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention (nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.d_k=d_model//num_heads\n",
    "\n",
    "        self.W_q=nn.Linear(d_model,d_model)\n",
    "        self.W_k=nn.Linear(d_model,d_model)\n",
    "        self.W_v=nn.Linear(d_model,d_model)\n",
    "        self.W_o=nn.Linear(d_model,d_model)\n",
    "    def forward(self,x,mask=None):\n",
    "        batch_size,seq_len,_=x.size()\n",
    "        Q=self.W_q(x).view(batch_size,seq_len,self.num_heads,self.d_k).transpose(1,2)\n",
    "        K=self.W_k(x).view(batch_size,seq_len,self.num_heads,self.d_k).transpose(1,2)\n",
    "        V=self.W_v(x).view(batch_size,seq_len,self.num_heads,self.d_k).transpose(1,2)\n",
    "\n",
    "        scores=torch.matmul(Q,K.transpose(-2,-1))/torch.sqrt(torch.tensor(self.d_k,dtype=torch.float32))\n",
    "        if mask is not None:\n",
    "            scores=scores.masked_fill(mask==0,float('-inf'))\n",
    "\n",
    "        attention_weights=F.softmax(scores,dim=-1)\n",
    "\n",
    "        context=torch.matmul(attention_weights,V)\n",
    "        context=context.transpose(1,2).contiguous()\n",
    "        context=context.view(batch_size,seq_len,self.d_model)\n",
    "\n",
    "        output=self.W_o(context)\n",
    "        return output\n",
    "if __name__==\"__main__\":\n",
    "    d_model=512\n",
    "    num_heads=8\n",
    "    batch_size=1\n",
    "    seq_len=5\n",
    "\n",
    "    mha=MultiHeadAttention(d_model,num_heads)\n",
    "    x=torch.randn(batch_size,seq_len,d_model)\n",
    "    output=mha(x)\n",
    "    print(x.shape)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29a6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "from collections import Counter, defaultdict\n",
    "import logging, pprint\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "class GraphAnalyzer:\n",
    "    def __init__(self, graph):\n",
    "        \"\"\"\n",
    "        :param graph: 直接传入 {\"nodes\": [...], \"edges\": [...]} 结构\n",
    "        \"\"\"\n",
    "        self.graph_json = graph  # 原始图数据\n",
    "        self.G = nx.Graph()\n",
    "\n",
    "        # 构建 NetworkX 图\n",
    "        for e in graph[\"edges\"]:\n",
    "            self.G.add_edge(e[\"source\"], e[\"target\"], weight=e.get(\"weight\", 1))\n",
    "\n",
    "        # 新增缓存字段\n",
    "        self.last_analysis = {}\n",
    "\n",
    "    def analyze_existing_graph(self, add_community_name=True, sort_by=\"degree\", sort_reverse=True):\n",
    "        colors = ['#065758', '#25767B', '#44949E', '#62B3C1', '#81D2E4', '#BCEDD8']\n",
    "        partition = community_louvain.best_partition(self.G)\n",
    "        community_size = Counter(partition.values())\n",
    "        degree = dict(self.G.degree())\n",
    "        degree_centrality = nx.degree_centrality(self.G)\n",
    "        betweenness = nx.betweenness_centrality(self.G)\n",
    "        clustering = nx.clustering(self.G)\n",
    "\n",
    "        rep_name = {}\n",
    "        if add_community_name:\n",
    "            for cid in community_size:\n",
    "                members = [n for n, c in partition.items() if c == cid]\n",
    "                hub = max(members, key=lambda n: degree[n])\n",
    "                node_obj = next(n for n in self.graph_json[\"nodes\"] if n[\"id\"] == hub)\n",
    "                rep_name[cid] = node_obj.get(\"name\", node_obj.get(\"label\", f\"Community_{cid}\"))\n",
    "\n",
    "        nodes_list = []\n",
    "        id2node = {n[\"id\"]: n for n in self.graph_json[\"nodes\"]}\n",
    "        for nid, cid in partition.items():\n",
    "            n = id2node[nid]\n",
    "            node_data = {\n",
    "                \"id\": nid,\n",
    "                \"community_id\": cid,\n",
    "                \"community_size\": community_size[cid],\n",
    "                \"degree\": degree[nid],\n",
    "                \"degree_centrality\": degree_centrality[nid],\n",
    "                \"betweenness\": betweenness[nid],\n",
    "                \"clustering\": clustering[nid],\n",
    "                \"style\": {\"fill\": colors[cid % len(colors)]}\n",
    "            }\n",
    "\n",
    "            if add_community_name:\n",
    "                node_data[\"community_name\"] = rep_name[cid]\n",
    "\n",
    "            for k, v in n.items():\n",
    "                if k not in node_data:\n",
    "                    node_data[k] = v\n",
    "\n",
    "            nodes_list.append(node_data)\n",
    "\n",
    "        metrics = {\n",
    "            \"partition\": partition,\n",
    "            \"community_size\": community_size,\n",
    "            \"degree\": degree,\n",
    "            \"degree_centrality\": degree_centrality,\n",
    "            \"betweenness\": betweenness,\n",
    "            \"clustering\": clustering,\n",
    "        }\n",
    "\n",
    "        if sort_by:\n",
    "            nodes_list.sort(key=lambda x: x.get(sort_by, 0), reverse=sort_reverse)\n",
    "\n",
    "        # 缓存分析结果\n",
    "        self.last_analysis = {\n",
    "            \"nodes_list\": nodes_list,\n",
    "            \"metrics\": metrics,\n",
    "            \"rep_name\": rep_name,\n",
    "            \"colors\": colors\n",
    "        }\n",
    "\n",
    "        return nodes_list, metrics\n",
    "\n",
    "    def community_analysis(self):\n",
    "        \"\"\"\n",
    "        基于最近一次 analyze_existing_graph 的结果，返回完整社区信息。\n",
    "        \"\"\"\n",
    "        if not self.last_analysis:\n",
    "            raise ValueError(\"请先调用 analyze_existing_graph\")\n",
    "\n",
    "        metrics = self.last_analysis[\"metrics\"]\n",
    "        rep_name = self.last_analysis.get(\"rep_name\")\n",
    "        colors = self.last_analysis.get(\"colors\")\n",
    "\n",
    "        community_size = metrics[\"community_size\"]\n",
    "\n",
    "        communities = []\n",
    "        for cid in sorted(community_size.keys()):\n",
    "            community = {\n",
    "                \"community_id\": cid,\n",
    "                \"community_size\": community_size[cid],\n",
    "                \"color\": colors[cid % len(colors)] if colors else \"#000000\"\n",
    "            }\n",
    "            if rep_name and cid in rep_name:\n",
    "                community[\"community_name\"] = rep_name[cid]\n",
    "            communities.append(community)\n",
    "\n",
    "        return communities\n",
    "\n",
    "    def community_analysis_new(self, classify_by_order=False):\n",
    "        \"\"\"\n",
    "        基于最近一次 analyze_existing_graph 的结果，返回按照一阶和二阶邻居划分的社区信息，并根据节点数量从高到低排序。\n",
    "\n",
    "        如果 classify_by_order=True，则返回分类为一阶和二阶的社区。\n",
    "        否则返回原始社区结构。\n",
    "        \"\"\"\n",
    "        if not self.last_analysis:\n",
    "            raise ValueError(\"请先调用 analyze_existing_graph\")\n",
    "\n",
    "        nodes_list = self.last_analysis[\"nodes_list\"]\n",
    "        # metrics = self.last_analysis[\"metrics\"]\n",
    "        rep_name = self.last_analysis.get(\"rep_name\")\n",
    "        colors = self.last_analysis.get(\"colors\")\n",
    "\n",
    "        # Step 1: 构建社区结构并收集成员\n",
    "        communities = defaultdict(list)\n",
    "        for node in nodes_list:\n",
    "            cid = node[\"community_id\"]\n",
    "            communities[cid].append(node)\n",
    "\n",
    "        # Step 2: 为每个社区找中心节点（度最大的节点）\n",
    "        center_map = {}\n",
    "        for cid, members in communities.items():\n",
    "            center = max(members, key=lambda n: n[\"degree\"])  # 根据 degree 找出中心节点\n",
    "            center_map[cid] = center[\"id\"]  # {cid: center_node_id}\n",
    "\n",
    "        # Step 3: 构建详细社区信息，并判断是否属于一阶或二阶\n",
    "        detailed_communities = []\n",
    "        first_order_communities = []\n",
    "        second_order_communities = []\n",
    "\n",
    "        for cid, members in communities.items():\n",
    "            center = center_map[cid]  # 使用真正的中心节点 ID\n",
    "\n",
    "            # 获取该中心节点的一阶、二阶邻居\n",
    "            first_neighbors = set(self.G.neighbors(center))\n",
    "            second_neighbors = set()\n",
    "            for neighbor in first_neighbors:\n",
    "                second_neighbors.update(self.G.neighbors(neighbor))\n",
    "            logging.debug(f\"{center=}\")\n",
    "            logging.debug(f\"(before cleanup) {second_neighbors=}\")\n",
    "\n",
    "            # 清理：去掉自己和重复项\n",
    "            second_neighbors -= first_neighbors\n",
    "            second_neighbors.discard(center)\n",
    "            \n",
    "            logging.debug(f\"(after  cleanup) {second_neighbors=}\")\n",
    "            assert center not in second_neighbors\n",
    "\n",
    "            # 社区成员 = 中心 + 一阶 + 二阶（理论上）\n",
    "            community_members = {center} | first_neighbors | second_neighbors\n",
    "            community_nodes = [node for node in nodes_list if node[\"id\"] in community_members]\n",
    "\n",
    "            # 实际社区成员（来自图算法划分的结果）\n",
    "            actual_community_member_ids = {node[\"id\"] for node in members}\n",
    "\n",
    "            # 判断是否有超出一阶邻居范围的成员\n",
    "            non_first_order_members = actual_community_member_ids - ({center} | first_neighbors)\n",
    "            is_second_order = len(non_first_order_members) > 0\n",
    "\n",
    "            # 构造社区对象\n",
    "            detailed_community = {\n",
    "                \"community_id\": cid,\n",
    "                \"community_size\": len(members),\n",
    "                \"center\": center,\n",
    "                \"color\": colors[cid % len(colors)] if colors else \"#000000\",\n",
    "                \"nodes\": community_nodes\n",
    "            }\n",
    "            if rep_name and cid in rep_name:\n",
    "                detailed_community[\"community_name\"] = rep_name[cid]\n",
    "\n",
    "            detailed_communities.append(detailed_community)\n",
    "\n",
    "            if is_second_order:\n",
    "                second_order_communities.append(detailed_community)\n",
    "            else:\n",
    "                first_order_communities.append(detailed_community)\n",
    "\n",
    "        # 按照社区大小排序\n",
    "        detailed_communities.sort(key=lambda x: x[\"community_size\"], reverse=True)\n",
    "        first_order_communities.sort(key=lambda x: x[\"community_size\"], reverse=True)\n",
    "        second_order_communities.sort(key=lambda x: x[\"community_size\"], reverse=True)\n",
    "\n",
    "        # 新增：统计一阶和二阶社区的总节点数和社区数量\n",
    "        first_order_total_nodes = sum(comm[\"community_size\"] for comm in first_order_communities)\n",
    "        second_order_total_nodes = sum(comm[\"community_size\"] for comm in second_order_communities)\n",
    "        first_order_community_count = len(first_order_communities)\n",
    "        second_order_community_count = len(second_order_communities)\n",
    "\n",
    "        # 如果需要分类返回\n",
    "        if classify_by_order:\n",
    "            return {\n",
    "                \"first_level_community\": first_order_communities[:10],  # Top10\n",
    "                \"second_level_community\": second_order_communities[:10],\n",
    "                \"first_order_total_nodes\": first_order_total_nodes,  # 一阶社区总节点数\n",
    "                \"second_order_total_nodes\": second_order_total_nodes,\n",
    "                \"first_order_community_count\": first_order_community_count,  # 一阶社区数量\n",
    "                \"second_order_community_count\": second_order_community_count\n",
    "            }\n",
    "\n",
    "        return detailed_communities\n",
    "\n",
    "    def edge_analysis(self, top_n=10):\n",
    "        \"\"\"\n",
    "        链接分析，返回图中边的信息。\n",
    "        \"\"\"\n",
    "        edges = sorted(\n",
    "            self.graph_json[\"edges\"],\n",
    "            key=lambda e: e.get(\"weight\", 1),\n",
    "            reverse=True\n",
    "        )[:top_n]\n",
    "\n",
    "        return {\n",
    "            \"total_edges\": len(self.graph_json[\"edges\"]),\n",
    "            \"top_weighted_edges\": edges\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
