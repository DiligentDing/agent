{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8318397",
   "metadata": {},
   "source": [
    "# Generating Q&A Pairs from UMLS Knowledge Graph Paths\n",
    "\n",
    "This notebook generates English question-answer pairs using multi-hop paths from the UMLS knowledge graph, where:\n",
    "- Questions require multi-hop professional reasoning to solve\n",
    "- Answers are typically the final entity in the path (with some exceptions where appropriate)\n",
    "- Answers include brief reasoning logic for evaluation purposes\n",
    "- Generated Q&A pairs are saved in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933a74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "from openai import AzureOpenAI\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Azure OpenAI设置\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"5a1437f6ff2648b9b969507fb5a73276\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ai-mistraleastus2753718354821.openai.azure.com/\"\n",
    "\n",
    "# 初始化Azure OpenAI客户端\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Configuration parameters\n",
    "MODEL = \"gpt-4.1-noah\"  # Using advanced model to ensure high-quality questions\n",
    "TEMPERATURE = 0.7       # Increased temperature for diversity\n",
    "RATE_LIMIT_S = 1.2      # Request interval to avoid API rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbac5933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2068 paths\n",
      "Template types: ['Disease_Drug_Target', 'Disease_Drug_moA']\n",
      "\n",
      "Sample path (Template: Disease_Drug_moA):\n",
      "path_strs: ['Granuloma inguinale', 'may_be_treated_by', 'Doxycycline anhydrous', 'has_mechanism_of_action', 'Protein Synthesis Inhibitors']\n"
     ]
    }
   ],
   "source": [
    "# Load merged_paths.json file\n",
    "def load_paths_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        paths_data = json.load(f)\n",
    "    return paths_data\n",
    "\n",
    "# Path and output files\n",
    "paths_file = Path(\"/home/xinding/dingxin/Agent/MAIA/code/merged_paths.json\")\n",
    "output_file = Path(\"/home/xinding/dingxin/Agent/MAIA/code/umls_qa_pairs_english.json\")  # English version\n",
    "\n",
    "# Load path data\n",
    "paths_data = load_paths_data(paths_file)\n",
    "print(f\"Loaded {sum(len(paths) for paths in paths_data.values())} paths\")\n",
    "\n",
    "# View data structure\n",
    "template_ids = list(paths_data.keys())\n",
    "print(f\"Template types: {template_ids}\")\n",
    "\n",
    "# Randomly select a path to view its structure\n",
    "template = random.choice(template_ids)\n",
    "if paths_data[template]:\n",
    "    sample_path = paths_data[template][0]\n",
    "    print(f\"\\nSample path (Template: {template}):\")\n",
    "    if 'path_strs' in sample_path:\n",
    "        print(f\"path_strs: {sample_path['path_strs']}\")\n",
    "    else:\n",
    "        print(\"Path structure:\", sample_path.keys())\n",
    "else:\n",
    "    print(f\"Template {template} has no available paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后共有 2068 条有效路径\n",
      "模板 Disease_Drug_Target: 568 条有效路径\n",
      "模板 Disease_Drug_moA: 1500 条有效路径\n"
     ]
    }
   ],
   "source": [
    "def filter_valid_paths(paths_data, min_path_length=3):\n",
    "    \"\"\"Filter valid paths - keep only those with complete structure and sufficient length\"\"\"\n",
    "    valid_paths = {}\n",
    "    \n",
    "    for template_id, paths in paths_data.items():\n",
    "        valid_template_paths = []\n",
    "        \n",
    "        for path in paths:\n",
    "            # 检查路径是否包含必要的字段并且长度足够\n",
    "            if ('path_strs' in path and \n",
    "                isinstance(path['path_strs'], list) and \n",
    "                len(path['path_strs']) >= min_path_length):\n",
    "                valid_template_paths.append(path)\n",
    "        \n",
    "        if valid_template_paths:\n",
    "            valid_paths[template_id] = valid_template_paths\n",
    "    \n",
    "    return valid_paths    \n",
    "    # Filter valid paths\n",
    "valid_paths = filter_valid_paths(paths_data)\n",
    "print(f\"After filtering, there are {sum(len(paths) for paths in valid_paths.values())} valid paths\")\n",
    "for template_id, paths in valid_paths.items():\n",
    "    print(f\"Template {template_id}: {len(paths)} valid paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_prompt(path_info, template_id):\n",
    "    \"\"\"Create a high-quality English prompt for generating complex medical Q-A pairs\"\"\"\n",
    "\n",
    "    path_strs = path_info[\"path_strs\"]\n",
    "    template_map = {\n",
    "        \"Disease_Drug_Target\": \"Disease → Drug → Target\",\n",
    "        \"Disease_Drug_moA\":    \"Disease → Drug → Mechanism-of-Action\"\n",
    "    }\n",
    "    template_desc = template_map.get(template_id, template_id)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a senior medical educator who must craft challenging Q&A pairs from UMLS multi-hop reasoning paths.\n",
    "\n",
    "Path type: {template_desc}\n",
    "UMLS path: {' -> '.join(path_strs)}\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Write a question that requires **multi-step professional reasoning** along this path.\n",
    "2. You may reveal **at most one or two** intermediate concepts as clues, but do NOT expose the full path.\n",
    "3. Frame the question in a realistic clinical, pharmacological, or research scenario—professional and concise, not overly detailed.\n",
    "4. The answer should normally be the terminal entity **\\\"{path_strs[-1]}\\\"**; if another node is more clinically sensible, use it and explain why.\n",
    "5. After the entity, add a **succinct (≤ 40 words) clinical/pharmacological rationale** introduced by a semicolon, so evaluators can easily judge correctness.\n",
    "6. Provide a short (20–40 words) “reasoning_path” summary showing the key medical logic without disclosing every node.\n",
    "7. Output **only** the following JSON structure—no extra text:\n",
    "\n",
    "{{\n",
    "  \"question\": \"...\",\n",
    "  \"answer\": \"<Entity>; <≤40-word rationale>\",\n",
    "  \"reasoning_path\": \"<20–40-word reasoning summary>\",\n",
    "  \"umls_path\": {path_strs},\n",
    "  \"template_id\": \"{template_id}\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8b86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pair(path_info, template_id):\n",
    "    \"\"\"Use OpenAI to generate Q&A pairs\"\"\"\n",
    "    prompt = create_qa_prompt(path_info, template_id)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical education expert specializing in creating high-quality medical reasoning questions in English.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # 尝试解析返回的JSON\n",
    "        try:\n",
    "            # 查找JSON部分 (可能会有额外的文本)\n",
    "            json_match = re.search(r'({[\\s\\S]*})', result)\n",
    "            if json_match:\n",
    "                result = json_match.group(1)\n",
    "            \n",
    "            qa_pair = json.loads(result)\n",
    "            \n",
    "            # Ensure required fields are present\n",
    "            if \"question\" not in qa_pair or \"answer\" not in qa_pair:\n",
    "                return None\n",
    "                \n",
    "            # Add original path data and metadata\n",
    "            qa_pair[\"umls_path\"] = path_info[\"path_strs\"]\n",
    "            qa_pair[\"template_id\"] = template_id\n",
    "            \n",
    "            return qa_pair\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse JSON: {result[:100]}...\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Q&A pair: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4294eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the number of samples per template (now this is no longer needed, as we generate for all paths)\n",
    "# samples_per_template = 50  # This is no longer needed\n",
    "output_qa_pairs = []\n",
    "\n",
    "# Iterate through all the templates and their associated paths\n",
    "for template_id, paths in valid_paths.items():\n",
    "    print(f\"Generating Q&A pairs for template '{template_id}'...\")\n",
    "    \n",
    "    # Iterate through all paths for this template (no random sampling)\n",
    "    for path in tqdm.tqdm(paths, desc=f\"Template: {template_id}\"):\n",
    "        qa_pair = generate_qa_pair(path, template_id)\n",
    "        \n",
    "        if qa_pair:\n",
    "            output_qa_pairs.append(qa_pair)\n",
    "            # Add a short delay to avoid API rate limits\n",
    "            time.sleep(RATE_LIMIT_S)\n",
    "            \n",
    "            # Save intermediate results every 10 samples\n",
    "            if len(output_qa_pairs) % 10 == 0:\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(output_qa_pairs, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"Saved {len(output_qa_pairs)} Q&A pairs\")\n",
    "\n",
    "# Save final results\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_qa_pairs, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Finished generating and saving {len(output_qa_pairs)} Q&A pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b092212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated and saved 100 Q&A pairs to /home/xinding/dingxin/Agent/MAIA/code/umls_qa_pairs_english.json\n",
      "\n",
      "Sample Q&A pairs:\n",
      "\n",
      "Example 1:\n",
      "Question: In patients with neutropenia who require oral prophylactic antibiotics, which class of antimicrobial agents is utilized due to its ability to inhibit bacterial DNA replication by targeting a specific bacterial enzyme?\n",
      "Answer: DNA Gyrase Inhibitors; because ciprofloxacin lactate, commonly used to prevent infections in neutropenic patients, acts by inhibiting bacterial DNA gyrase, thereby blocking DNA replication.\n",
      "Reasoning Path: Neutropenia increases infection risk, often necessitating prophylactic antibiotics such as ciprofloxacin lactate. This drug's efficacy is due to its mechanism of action as a DNA gyrase inhibitor, which prevents bacterial DNA replication.\n",
      "UMLS Path: neutropenia -> may_be_treated_by -> ciprofloxacin lactate -> has_mechanism_of_action -> DNA Gyrase Inhibitors\n",
      "\n",
      "Example 2:\n",
      "Question: Which molecular target is inhibited by a medication commonly prescribed to lower elevated lipoprotein levels in patients with hyperlipoproteinemia, thereby reducing cholesterol biosynthesis?\n",
      "Answer: 3-Hydroxy-3-Methylglutaryl-Coenzyme A Reductase; because statins like pravastatin sodium are used to treat hyperlipoproteinemia by inhibiting this enzyme, leading to decreased cholesterol synthesis.\n",
      "Reasoning Path: Hyperlipoproteinemia is treated with pravastatin sodium, which exerts its effect by targeting and inhibiting 3-Hydroxy-3-Methylglutaryl-Coenzyme A Reductase.\n",
      "UMLS Path: Hyperlipoproteinemia -> may_be_treated_by -> Pravastatin Sodium -> has_target -> 3-Hydroxy-3-Methylglutaryl-Coenzyme A Reductase\n",
      "\n",
      "Example 3:\n",
      "Question: In a patient with hypertriglyceridemia who is prescribed a pravastatin-containing therapy, which enzyme is primarily inhibited to achieve the lipid-lowering effect?\n",
      "Answer: 3-Hydroxy-3-Methylglutaryl-Coenzyme A Reductase; because pravastatin exerts its lipid-lowering effect by inhibiting this rate-limiting enzyme in hepatic cholesterol synthesis, which is key in the management of hypertriglyceridemia.\n",
      "Reasoning Path: Hypertriglyceridemia is treated with statins such as pravastatin, which act by inhibiting their primary molecular target, HMG-CoA reductase, thereby reducing cholesterol synthesis and improving lipid profiles.\n",
      "UMLS Path: Hypertriglyceridemia -> may_be_treated_by -> Pravastatin-containing product -> has_target -> 3-Hydroxy-3-Methylglutaryl-Coenzyme A Reductase\n"
     ]
    }
   ],
   "source": [
    "# Save final results\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_qa_pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Successfully generated and saved {len(output_qa_pairs)} Q&A pairs to {output_file}\")\n",
    "\n",
    "# Display some examples\n",
    "print(\"\\nSample Q&A pairs:\")\n",
    "for i, qa in enumerate(random.sample(output_qa_pairs, min(3, len(output_qa_pairs)))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {qa['question']}\")\n",
    "    print(f\"Answer: {qa['answer']}\")\n",
    "    print(f\"Reasoning Path: {qa.get('reasoning_path', 'N/A')}\")\n",
    "    print(f\"UMLS Path: {' -> '.join(qa['umls_path'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7896dab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Q&A Data Analysis ===\n",
      "Total Q&A pairs: 100\n",
      "Answers with reasoning: 100 (100.0%)\n",
      "\n",
      "Template distribution:\n",
      "  Disease_Drug_Target: 50 (50.0%)\n",
      "  Disease_Drug_moA: 50 (50.0%)\n",
      "\n",
      "Question length: Average 202.9 characters, Range 125-313 characters\n",
      "Answer length: Average 194.7 characters, Range 140-279 characters\n"
     ]
    }
   ],
   "source": [
    "# Analyze generated Q&A pairs\n",
    "def analyze_qa_pairs(qa_pairs):\n",
    "    \"\"\"Analyze the generated Q&A data\"\"\"\n",
    "    \n",
    "    template_counts = defaultdict(int)\n",
    "    question_lengths = []\n",
    "    answer_lengths = []\n",
    "    has_reasoning = 0\n",
    "    \n",
    "    for qa in qa_pairs:\n",
    "        template_counts[qa.get('template_id', 'unknown')] += 1\n",
    "        question_lengths.append(len(qa.get('question', '')))\n",
    "        answer_lengths.append(len(qa.get('answer', '')))\n",
    "        \n",
    "        # Check if answer includes reasoning (contains semicolon)\n",
    "        if ';' in qa.get('answer', ''):\n",
    "            has_reasoning += 1\n",
    "    \n",
    "    print(\"=== Q&A Data Analysis ===\")\n",
    "    print(f\"Total Q&A pairs: {len(qa_pairs)}\")\n",
    "    print(f\"Answers with reasoning: {has_reasoning} ({has_reasoning/len(qa_pairs):.1%})\")\n",
    "    \n",
    "    print(\"\\nTemplate distribution:\")\n",
    "    for template, count in template_counts.items():\n",
    "        print(f\"  {template}: {count} ({count/len(qa_pairs):.1%})\")\n",
    "    \n",
    "    if question_lengths:\n",
    "        print(f\"\\nQuestion length: Average {sum(question_lengths)/len(question_lengths):.1f} characters, \" \n",
    "              f\"Range {min(question_lengths)}-{max(question_lengths)} characters\")\n",
    "    \n",
    "    if answer_lengths:\n",
    "        print(f\"Answer length: Average {sum(answer_lengths)/len(answer_lengths):.1f} characters, \"\n",
    "              f\"Range {min(answer_lengths)}-{max(answer_lengths)} characters\")\n",
    "\n",
    "# 分析生成的数据\n",
    "analyze_qa_pairs(output_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd39319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 生成 2068 条 → enriched_umls_qa.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "merge_umls_paths_get_related_w_lookup.py\n",
    "---------------------------------------\n",
    "• 读   umls_qa_pairs.json\n",
    "• 读   merged_paths.json\n",
    "• 输出 enriched_umls_qa.json\n",
    "  - id           sha256(question)[:16]\n",
    "  - tool_calls   [umls.concept_lookup, umls.get_related, ...]\n",
    "\"\"\"\n",
    "\n",
    "import json, hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "QA_FILE   = \"umls_qa_pairs.json\"\n",
    "PATH_FILE = \"merged_paths.json\"\n",
    "OUT_FILE  = \"enriched_umls_qa.json\"\n",
    "\n",
    "# ========== util ==========\n",
    "def sha16(txt: str) -> str:\n",
    "    return hashlib.sha256(txt.encode()).hexdigest()[:16]\n",
    "\n",
    "def build_tool_calls(entity_name: str,\n",
    "                     cuis: List[str],\n",
    "                     relas: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    构造调用序列：\n",
    "      0) umls.concept_lookup  (name → CUI0)\n",
    "      1) umls.get_related     (CUI0 → CUI1)\n",
    "      2) umls.get_related     (CUI1 → CUI2) ...\n",
    "      N) umls.concept_lookup  (CUI_last → name)\n",
    "    \"\"\"\n",
    "    calls = [\n",
    "        {\n",
    "            \"tool\": \"umls.concept_lookup\",      # 名称查 CUI\n",
    "            \"params\": {\"name\": entity_name}\n",
    "        }\n",
    "    ]\n",
    "    for i, rela in enumerate(relas):\n",
    "        calls.append({\n",
    "            \"tool\": \"umls.get_related\",\n",
    "            \"params\": {\n",
    "                \"from_cui\": cuis[i],   # CUI_i\n",
    "                \"rela\": rela\n",
    "            }\n",
    "        })\n",
    "    # 最后补一条 CUI->name\n",
    "    calls.append({\n",
    "        \"tool\": \"umls.cui_to_name\",  # CUI_last 查名称\n",
    "        \"params\": {\n",
    "            \"cui\": cuis[-1]\n",
    "        }\n",
    "    })\n",
    "    return calls\n",
    "\n",
    "\n",
    "# ========== 1. 读取 ==========\n",
    "qa_pairs  = json.loads(Path(QA_FILE).read_text(encoding=\"utf-8\"))\n",
    "path_meta = json.loads(Path(PATH_FILE).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "#  (template_id, tuple(path_strs)) → (cuis, relas)\n",
    "path_dict: Dict[Tuple[str, tuple], Tuple[List[str], List[str]]] = {}\n",
    "for tmpl, lst in path_meta.items():\n",
    "    for entry in lst:\n",
    "        path_dict[(tmpl, tuple(entry[\"path_strs\"]))] = (\n",
    "            entry[\"cuis\"], entry[\"relas\"]\n",
    "        )\n",
    "\n",
    "# ========== 2. 合并 ==========\n",
    "enriched = []\n",
    "for qa in qa_pairs:\n",
    "    key = (qa[\"template_id\"], tuple(qa[\"umls_path\"]))\n",
    "    if key not in path_dict:\n",
    "        continue\n",
    "\n",
    "    cuis, relas = path_dict[key]\n",
    "    first_entity_name = qa[\"umls_path\"][0]        # e.g. \"Gynecomastia\"\n",
    "\n",
    "    enriched.append({\n",
    "        \"id\":         sha16(qa[\"question\"]),\n",
    "        \"question\":   qa[\"question\"],\n",
    "        \"tool_calls\": build_tool_calls(first_entity_name, cuis, relas),\n",
    "        \"answer\":     qa[\"answer\"],\n",
    "        \"reasoning_path\": qa[\"reasoning_path\"],\n",
    "        \"umls_path\":      qa[\"umls_path\"],\n",
    "        \"template_id\":    qa[\"template_id\"]\n",
    "    })\n",
    "\n",
    "# ========== 3. 写出 ==========\n",
    "Path(OUT_FILE).write_text(\n",
    "    json.dumps({\"dataset\": enriched}, ensure_ascii=False, indent=2)\n",
    ")\n",
    "print(f\"✅ 生成 {len(enriched)} 条 → {OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2429b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据库连接成功!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pymysql as mysql\n",
    "\n",
    "class DataBase:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.DB = mysql.connect(\n",
    "                host=\"server.acemap.cn\",\n",
    "                user=\"groupleader\",\n",
    "                passwd=\"onlyleaders\",\n",
    "                port=13306,\n",
    "                database=\"mag-new-160205\",\n",
    "                charset=\"utf8\"\n",
    "            )\n",
    "            print('数据库连接成功!')\n",
    "            self.cursor = self.DB.cursor()\n",
    "        except mysql.Error as e:\n",
    "            print('数据库连接失败原因:' + str(e))\n",
    "\n",
    "    def commit(self):\n",
    "        self.DB.commit()\n",
    "        \n",
    "    def rollback(self):\n",
    "        self.DB.rollback()\n",
    "\n",
    "    def close(self):\n",
    "        self.DB.close()\n",
    "\n",
    "    def insert_paper(self, paper_id, title, keywords, authors, abstract, year, publication, new_acemap_id):\n",
    "        try:\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO new_ccfa (paper_id, title, keywords, authors, abstract, year, publication, new_acemap_id)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            self.cursor.execute(sql, (paper_id, title, keywords, authors, abstract, year, publication, new_acemap_id))\n",
    "            self.commit()\n",
    "        except mysql.Error as e:\n",
    "            print('插入数据失败原因:' + str(e))\n",
    "            self.rollback()\n",
    "\n",
    "db = DataBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76e10272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据库连接成功!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 56.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pymysql as mysql\n",
    "\n",
    "class DataBase:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.DB = mysql.connect(\n",
    "                host=\"server.acemap.cn\",\n",
    "                user=\"groupleader\",\n",
    "                passwd=\"onlyleaders\",\n",
    "                port=13306,\n",
    "                database=\"mag-new-160205\",\n",
    "                charset=\"utf8\"\n",
    "            )\n",
    "            print('数据库连接成功!')\n",
    "            self.cursor = self.DB.cursor()\n",
    "        except mysql.MySQLError as e:\n",
    "            print('数据库连接失败原因:' + str(e))\n",
    "\n",
    "    def commit(self):\n",
    "        self.DB.commit()\n",
    "        \n",
    "    def rollback(self):\n",
    "        self.DB.rollback()\n",
    "\n",
    "    def close(self):\n",
    "        self.DB.close()\n",
    "\n",
    "    def insert_paper(self, paper_id, title, keywords, authors, abstract, year, publication, new_acemap_id):\n",
    "        try:\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO new_ccfa (paper_id, title, keywords, authors, abstract, year, publication, new_acemap_id)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            self.cursor.execute(sql, (paper_id, title, keywords, authors, abstract, year, publication, new_acemap_id))\n",
    "            self.commit()\n",
    "        except mysql.MySQLError as e:\n",
    "            print('插入数据失败原因:' + str(e))\n",
    "            self.rollback()\n",
    "\n",
    "# 正确读取csv（处理BOM，字段自动对齐，丢弃多余字段）\n",
    "df = pd.read_csv('tmp.csv', encoding='utf-8-sig', dtype=str)\n",
    "df = df.fillna('')  # 避免None插入出错\n",
    "\n",
    "db = DataBase()\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # 只取需要的8列，顺序严格与insert_paper参数对齐\n",
    "    db.insert_paper(\n",
    "        row['paper_id'],\n",
    "        row['title'],\n",
    "        row['keywords'],\n",
    "        row['authors'],\n",
    "        row['abstract'],\n",
    "        row['year'],\n",
    "        row['publication'],\n",
    "        row['new_acemap_id']\n",
    "    )\n",
    "\n",
    "db.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
