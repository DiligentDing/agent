{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef9860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, pathlib, tqdm\n",
    "from openai import AzureOpenAI\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-4jnd9yjoIXnQRQ5SXR2b3bVO1d3sHtuyegGMzAl6awSWDRNn' \n",
    "os.environ['OPENAI_BASE_URL'] = 'https://api2.aigcbest.top/v1' \n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-OlimLcefr3MBSt08IrcZ9LrhP94qqni4w3u4qkOPFtAULcDD' \n",
    "# os.environ['OPENAI_BASE_URL'] = 'https://api.chatanywhere.tech' \n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"5a1437f6ff2648b9b969507fb5a73276\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ai-mistraleastus2753718354821.openai.azure.com/\"\n",
    "# ========= 0. Azure OpenAI 配置 =========\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "RESPONDER_MODEL = \"gpt-4.1-noah\"          # 回答模型\n",
    "JUDGE_MODEL     = \"gpt-4.1-noah\"          # 亦可用同一模型评分\n",
    "TEMP            = 0.1               # 回答温度\n",
    "RATE_LIMIT_S    = 1.2      \n",
    "out_dir = pathlib.Path(\"../res/MAIA\")\n",
    "out_dir.mkdir(exist_ok=True)         # 简单限流间隔\n",
    "with open(\"../benchmark/MAIA_reasoning.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_pairs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1400fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 100/100 [00:00<00:00, 838860.80it/s]\n",
      "Judging answers: 100%|██████████| 100/100 [05:47<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Summary ===\n",
      "Samples evaluated : 100/1\n",
      "Average score     : 2.50 / 5\n",
      "Score distribution: {1: 4, 2: 72, 3: 9, 5: 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ========= 1. 加载数据 =========\n",
    "with open(\"../MAIA/MAIA_retrieval.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_pairs = json.load(f)\n",
    "\n",
    "# ========= 2. 生成模型答案 =========\n",
    "answers_path = out_dir / \"model_answers.json\"\n",
    "if answers_path.exists():\n",
    "    model_answers = json.load(open(answers_path))\n",
    "else:\n",
    "    model_answers = {}\n",
    "\n",
    "for idx, qa in enumerate(tqdm.tqdm(qa_pairs['dataset'], desc=\"Generating answers\")):\n",
    "    q = qa[\"question\"]\n",
    "    if str(idx) in model_answers:\n",
    "        continue  # 已存在则跳过（断点续跑）\n",
    "\n",
    "    try:\n",
    "        # client = OpenAI(\n",
    "        #     api_key=\"sk-5157bc95a7ee4f7e9086a80fd41c69fc\",\n",
    "        #     base_url=\"https://api.deepseek.com/v1\"\n",
    "        # )\n",
    "        # client = OpenAI()\n",
    "        resp = client.chat.completions.create(\n",
    "            model=RESPONDER_MODEL,\n",
    "            # temperature=TEMP,\n",
    "            # max_tokens=512,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an experienced oncologist answering exam-style clinical questions concisely and accurately.\"},\n",
    "                {\"role\": \"user\",    \"content\": q}\n",
    "            ],\n",
    "        )\n",
    "        model_answers[str(idx)] = resp.choices[0].message.content.strip()\n",
    "        time.sleep(RATE_LIMIT_S)\n",
    "    except Exception as e:\n",
    "        print(f\"[Responder error @ {idx}] {e}. Retrying in 10 s…\")\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "\n",
    "    # 每 10 条保存一次\n",
    "    if idx % 10 == 9:\n",
    "        json.dump(model_answers, open(answers_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# 最终保存\n",
    "json.dump(model_answers, open(answers_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "# ========= 3. 调用 Judge 评分 =========\n",
    "import re\n",
    "scores_path = out_dir / \"judge_scores.json\"\n",
    "if scores_path.exists():\n",
    "    judge_scores = json.load(open(scores_path))\n",
    "else:\n",
    "    judge_scores = {}\n",
    "\n",
    "judge_prompt_tpl = \"\"\"You are an impartial medical board examiner.\n",
    "Score the model answer against the reference answer on a 0–5 scale,\n",
    "using the *refined* rubric below.  If unsure between two scores, pick\n",
    "the **lower** one.\n",
    "\n",
    "Rubric:\n",
    "5 = Covers **all** key clinical facts in the reference; any additional\n",
    "    explanations are factually correct *and clinically relevant*; no\n",
    "    inaccuracies, unsafe statements, or major omissions.\n",
    "4 = ≥90 % of key facts correct; extra content is correct; at most one\n",
    "    minor omission **or** wording inaccuracy that does not alter meaning.\n",
    "3 = 70-89 % of key facts covered; may include a few minor errors or\n",
    "    omissions, but no clinically dangerous advice.\n",
    "2 = 40-69 % of key facts **or** ≥1 moderate factual error/omission; some\n",
    "    irrelevant or redundant statements allowed.\n",
    "1 = <40 % of key facts **or** major inaccuracies; content mostly\n",
    "    irrelevant or confusing.\n",
    "0 = Blank, nonsense, or any clearly unsafe recommendation.\n",
    "\n",
    "Penalty rules:\n",
    "• Extra content that is factually correct & relevant → **no penalty**.\n",
    "• Extra but irrelevant OR factually wrong content → lower the score.\n",
    "• Any unsafe or potentially harmful statement → max score = 1.\n",
    "\n",
    "Return exactly one line:\n",
    "\"<score 0-5>: <concise 1–2 sentence justification>\"\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference answer:\n",
    "{ref_answer}\n",
    "\n",
    "Model answer:\n",
    "{model_answer}\n",
    "\"\"\"\n",
    "\n",
    "for idx, qa in enumerate(tqdm.tqdm(qa_pairs['dataset'], desc=\"Judging answers\")):\n",
    "    if str(idx) in judge_scores:\n",
    "        continue\n",
    "\n",
    "    prompt = judge_prompt_tpl.format(\n",
    "        question=qa[\"question\"],\n",
    "        ref_answer=qa[\"answer\"],\n",
    "        model_answer=model_answers.get(str(idx), \"\")\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-12-01-preview\",\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=JUDGE_MODEL,\n",
    "            temperature=0,\n",
    "            max_tokens=120,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert clinical examiner.\"},\n",
    "                {\"role\": \"user\",    \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        # raw = resp.choices[0].message.content.strip()\n",
    "        # score = raw.split()[0]  # 第一词应该是分数\n",
    "        # judge_scores[str(idx)] = {\"score\": float(score), \"explanation\": raw}\n",
    "\n",
    "        raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "        m = re.search(r\"\\b([0-5](?:\\.\\d+)?)\\b\", raw)\n",
    "        if not m:\n",
    "            print(f\"[Judge format error @ {idx}] {raw}\")\n",
    "            continue                      # 或者 retry\n",
    "\n",
    "        score = float(m.group(1))\n",
    "        judge_scores[str(idx)] = {\"score\": score, \"explanation\": raw}\n",
    "        time.sleep(RATE_LIMIT_S)\n",
    "    except Exception as e:\n",
    "        print(f\"[Judge error @ {idx}] {e}. Retrying in 5 s…\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    if idx % 10 == 9:\n",
    "        json.dump(judge_scores, open(scores_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "json.dump(judge_scores, open(scores_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# ========= 4. 汇总统计 =========\n",
    "scores = [v[\"score\"] for v in judge_scores.values()]\n",
    "avg   = sum(scores) / len(scores)\n",
    "dist  = defaultdict(int)\n",
    "for s in scores:\n",
    "    dist[int(s)] += 1\n",
    "\n",
    "print(\"\\n=== Evaluation Summary ===\")\n",
    "print(f\"Samples evaluated : {len(scores)}/{len(qa_pairs)}\")\n",
    "print(f\"Average score     : {avg:.2f} / 5\")\n",
    "print(\"Score distribution:\", dict(sorted(dist.items())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b86774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12731b22cab448298b7b838a3e06e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/890 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae17405bb1d489caff18a006baf0791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "full-00000-of-00001.parquet:   0%|          | 0.00/437k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1a17db26e9489d9bef5dcb86cc1d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating full split:   0%|          | 0/1014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'ret_cacfe0e74802', 'question': 'What is the PMID of the article titled “[Practical guideline for short bowel syndrome]” by first author Dabsch S, published in Zeitschrift fur Gastroenterologie in 2025?', 'tool_calls': {'tool': ['pubmed.search'], 'params': ['{\"term\":\"\\\\\"[Practical guideline for short bowel syndrome].\\\\\"[ti] AND Dabsch S[au] AND 2025[dp]\",\"retmax\":1}']}, 'answer': ['40360142'], 'type': 'retrieval'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"DiligentDing/MAIA\", split=\"full\")  # loads the entire benchmark\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eaaa9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "! git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d43dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 3. 调用 Judge 评分 =========\n",
    "import re\n",
    "scores_path = out_dir / \"judge_scores.json\"\n",
    "if scores_path.exists():\n",
    "    judge_scores = json.load(open(scores_path))\n",
    "else:\n",
    "    judge_scores = {}\n",
    "\n",
    "judge_prompt_tpl = \"\"\"You are an impartial medical board examiner.\n",
    "Score the model answer against the reference answer on a 0–5 scale,\n",
    "using the *refined* rubric below.  If unsure between two scores, pick\n",
    "the **lower** one.\n",
    "\n",
    "Rubric:\n",
    "5 = Covers **all** key clinical facts in the reference; any additional\n",
    "    explanations are factually correct *and clinically relevant*; no\n",
    "    inaccuracies, unsafe statements, or major omissions.\n",
    "4 = ≥90 % of key facts correct; extra content is correct; at most one\n",
    "    minor omission **or** wording inaccuracy that does not alter meaning.\n",
    "3 = 70-89 % of key facts covered; may include a few minor errors or\n",
    "    omissions, but no clinically dangerous advice.\n",
    "2 = 40-69 % of key facts **or** ≥1 moderate factual error/omission; some\n",
    "    irrelevant or redundant statements allowed.\n",
    "1 = <40 % of key facts **or** major inaccuracies; content mostly\n",
    "    irrelevant or confusing.\n",
    "0 = Blank, nonsense, or any clearly unsafe recommendation.\n",
    "\n",
    "Penalty rules:\n",
    "• Extra content that is factually correct & relevant → **no penalty**.\n",
    "• Extra but irrelevant OR factually wrong content → lower the score.\n",
    "• Any unsafe or potentially harmful statement → max score = 1.\n",
    "\n",
    "Return exactly one line:\n",
    "\"<score 0-5>: <concise 1–2 sentence justification>\"\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference answer:\n",
    "{ref_answer}\n",
    "\n",
    "Model answer:\n",
    "{model_answer}\n",
    "\"\"\"\n",
    "\n",
    "for idx, qa in enumerate(tqdm.tqdm(qa_pairs['dataset'][:100], desc=\"Judging answers\")):\n",
    "    if str(idx) in judge_scores:\n",
    "        continue\n",
    "\n",
    "    prompt = judge_prompt_tpl.format(\n",
    "        question=qa[\"question\"],\n",
    "        ref_answer=qa[\"answer\"],\n",
    "        model_answer=model_answers.get(str(idx), \"\")\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-12-01-preview\",\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=JUDGE_MODEL,\n",
    "            temperature=0,\n",
    "            max_tokens=120,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert clinical examiner.\"},\n",
    "                {\"role\": \"user\",    \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        # raw = resp.choices[0].message.content.strip()\n",
    "        # score = raw.split()[0]  # 第一词应该是分数\n",
    "        # judge_scores[str(idx)] = {\"score\": float(score), \"explanation\": raw}\n",
    "\n",
    "        raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "        m = re.search(r\"\\b([0-5](?:\\.\\d+)?)\\b\", raw)\n",
    "        if not m:\n",
    "            print(f\"[Judge format error @ {idx}] {raw}\")\n",
    "            continue                      # 或者 retry\n",
    "\n",
    "        score = float(m.group(1))\n",
    "        judge_scores[str(idx)] = {\"score\": score, \"explanation\": raw}\n",
    "        time.sleep(RATE_LIMIT_S)\n",
    "    except Exception as e:\n",
    "        print(f\"[Judge error @ {idx}] {e}. Retrying in 5 s…\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    if idx % 10 == 9:\n",
    "        json.dump(judge_scores, open(scores_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "json.dump(judge_scores, open(scores_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# ========= 4. 汇总统计 =========\n",
    "scores = [v[\"score\"] for v in judge_scores.values()]\n",
    "avg   = sum(scores) / len(scores)\n",
    "dist  = defaultdict(int)\n",
    "for s in scores:\n",
    "    dist[int(s)] += 1\n",
    "\n",
    "print(\"\\n=== Evaluation Summary ===\")\n",
    "print(f\"Samples evaluated : {len(scores)}/{len(qa_pairs)}\")\n",
    "print(f\"Average score     : {avg:.2f} / 5\")\n",
    "print(\"Score distribution:\", dict(sorted(dist.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef8ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read /home/xinding/dingxin/Agent/MAIA/evaluation/noah/model_answers.json\n",
    "with open(\"../evaluation/noah/model_answers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    model_answers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "008519d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Judging answers: 100%|██████████| 55/55 [03:28<00:00,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Summary ===\n",
      "Samples evaluated : 55/169\n",
      "Average score     : 3.62 / 5\n",
      "Score distribution: {2: 6, 3: 14, 4: 30, 5: 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 调用 Judge 评分 =========\n",
    "import re\n",
    "scores_path = out_dir / \"judge_scores.json\"\n",
    "if scores_path.exists():\n",
    "    judge_scores = json.load(open(scores_path))\n",
    "else:\n",
    "    judge_scores = {}\n",
    "\n",
    "judge_prompt_tpl = \"\"\"You are an impartial medical board examiner.\n",
    "Score the following model answer against the reference answer on a scale of 0–5,\n",
    "using this rubric:\n",
    "5 = identical clinical content, no major omissions or errors;\n",
    "4 = nearly complete, minor omissions;\n",
    "3 = partial, some omissions or inaccuracies;\n",
    "2 = limited coverage or notable errors;\n",
    "1 = largely incorrect or irrelevant;\n",
    "0 = blank or nonsense.\n",
    "\n",
    "Only reply with the score (0–5) followed by a short 1–2 sentence justification.\n",
    "\n",
    "For example, \"4: The model answer is almost identical to the reference answer, but it misses a few minor details.\"\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference answer:\n",
    "{ref_answer}\n",
    "\n",
    "Model answer:\n",
    "{model_answer}\n",
    "\"\"\"\n",
    "\n",
    "for idx, qa in enumerate(tqdm.tqdm(qa_pairs[:55], desc=\"Judging answers\")):\n",
    "    if str(idx) in judge_scores:\n",
    "        continue\n",
    "\n",
    "    prompt = judge_prompt_tpl.format(\n",
    "        question=qa[\"question\"],\n",
    "        ref_answer=qa[\"answer\"],\n",
    "        model_answer=model_answers.get(str(idx), \"\")\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-12-01-preview\",\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=JUDGE_MODEL,\n",
    "            temperature=0,\n",
    "            max_tokens=120,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert clinical examiner.\"},\n",
    "                {\"role\": \"user\",    \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        # raw = resp.choices[0].message.content.strip()\n",
    "        # score = raw.split()[0]  # 第一词应该是分数\n",
    "        # judge_scores[str(idx)] = {\"score\": float(score), \"explanation\": raw}\n",
    "\n",
    "        raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "        m = re.search(r\"\\b([0-5](?:\\.\\d+)?)\\b\", raw)\n",
    "        if not m:\n",
    "            print(f\"[Judge format error @ {idx}] {raw}\")\n",
    "            continue                      # 或者 retry\n",
    "\n",
    "        score = float(m.group(1))\n",
    "        judge_scores[str(idx)] = {\"score\": score, \"explanation\": raw}\n",
    "        time.sleep(RATE_LIMIT_S)\n",
    "    except Exception as e:\n",
    "        print(f\"[Judge error @ {idx}] {e}. Retrying in 5 s…\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    if idx % 10 == 9:\n",
    "        json.dump(judge_scores, open(scores_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "json.dump(judge_scores, open(scores_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# ========= 4. 汇总统计 =========\n",
    "scores = [v[\"score\"] for v in judge_scores.values()]\n",
    "avg   = sum(scores) / len(scores)\n",
    "dist  = defaultdict(int)\n",
    "for s in scores:\n",
    "    dist[int(s)] += 1\n",
    "\n",
    "print(\"\\n=== Evaluation Summary ===\")\n",
    "print(f\"Samples evaluated : {len(scores)}/{len(qa_pairs)}\")\n",
    "print(f\"Average score     : {avg:.2f} / 5\")\n",
    "print(\"Score distribution:\", dict(sorted(dist.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95d46de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers (Claude): 100%|██████████| 100/100 [15:14<00:00,  9.15s/it]\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 0. 依赖与配置\n",
    "###############################################################################\n",
    "import os, json, time, pathlib, re, tqdm\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import httpx                         # 新增\n",
    "# from openai import AzureOpenAI      # 仍用于 Judge（若保留 GPT-4.1-noah）\n",
    "\n",
    "# Claude REST 端点 & 令牌\n",
    "CLAUDE_URL   = \"https://test.noahai.co/api/claude/\"\n",
    "CLAUDE_TOKEN = \"Token ab2af44c17490f0c3c3b221b0f6fc2c20d62590a\"\n",
    "\n",
    "RATE_LIMIT_S = 1.2                   # Claude 同样限流\n",
    "TEMP        = 0.1\n",
    "\n",
    "###############################################################################\n",
    "# 1. 加载数据\n",
    "###############################################################################\n",
    "with open(\"../dataset/sampled_qa.json\", encoding=\"utf-8\") as f:\n",
    "    qa_pairs = json.load(f)\n",
    "\n",
    "out_dir = pathlib.Path(\"../evaluation\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "answers_path = out_dir / \"model_answers.json\"\n",
    "\n",
    "###############################################################################\n",
    "# 2. Claude 生成答案\n",
    "###############################################################################\n",
    "def claude_call(user_prompt: str,\n",
    "                system_prompt: str = \"You are an experienced oncologist answering exam-style clinical questions concisely and accurately.\",\n",
    "                temperature: float = TEMP) -> str:\n",
    "    \"\"\"流式调用 Claude，并返回完整响应文本\"\"\"\n",
    "    data = {\"user_prompt\": user_prompt,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"temperature\": temperature}\n",
    "    headers = {\"Content-Type\": \"application/json\",\n",
    "               \"Authorization\": CLAUDE_TOKEN}\n",
    "\n",
    "    buffer = StringIO()\n",
    "    with httpx.Client(timeout=60) as client:\n",
    "        with client.stream(\"POST\", CLAUDE_URL, headers=headers, json=data) as r:\n",
    "            for chunk in r.iter_text():\n",
    "                buffer.write(chunk)\n",
    "    return buffer.getvalue().strip()\n",
    "\n",
    "# 断点续跑\n",
    "model_answers = json.load(open(answers_path)) if answers_path.exists() else {}\n",
    "\n",
    "for idx, qa in enumerate(tqdm.tqdm(qa_pairs, desc=\"Generating answers (Claude)\")):\n",
    "    if str(idx) in model_answers:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        answer_text = claude_call(qa[\"question\"])\n",
    "        model_answers[str(idx)] = answer_text\n",
    "        time.sleep(RATE_LIMIT_S)\n",
    "    except Exception as e:\n",
    "        print(f\"[Claude error @ {idx}] {e}. Retrying in 10 s…\")\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "\n",
    "    # 每 10 条保存一次\n",
    "    if idx % 10 == 9:\n",
    "        json.dump(model_answers, open(answers_path, \"w\", encoding=\"utf-8\"),\n",
    "                  ensure_ascii=False, indent=2)\n",
    "\n",
    "# 最终保存\n",
    "json.dump(model_answers, open(answers_path, \"w\", encoding=\"utf-8\"),\n",
    "          ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0491e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 100/100 [00:00<00:00, 773856.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, pathlib, tqdm\n",
    "from openai import AzureOpenAI\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-4jnd9yjoIXnQRQ5SXR2b3bVO1d3sHtuyegGMzAl6awSWDRNn' \n",
    "os.environ['OPENAI_BASE_URL'] = 'https://api2.aigcbest.top/v1' \n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-OlimLcefr3MBSt08IrcZ9LrhP94qqni4w3u4qkOPFtAULcDD' \n",
    "# os.environ['OPENAI_BASE_URL'] = 'https://api.chatanywhere.tech' \n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"5a1437f6ff2648b9b969507fb5a73276\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ai-mistraleastus2753718354821.openai.azure.com/\"\n",
    "# ========= 0. Azure OpenAI 配置 =========\n",
    "# client = AzureOpenAI(\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "#     api_version=\"2024-12-01-preview\",\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# )\n",
    "\n",
    "RESPONDER_MODEL = \"deepseek-reasoner\"          # 回答模型\n",
    "JUDGE_MODEL     = \"gpt-4.1-noah\"          # 亦可用同一模型评分\n",
    "TEMP            = 0.1               # 回答温度\n",
    "out_dir = pathlib.Path(\"../res/deepseek-v3\")\n",
    "out_dir.mkdir(exist_ok=True)         # 简单限流间隔\n",
    "\n",
    "# ========= 1. 加载数据 =========\n",
    "with open(\"../MAIA/MAIA_retrieval.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_pairs = json.load(f)\n",
    "\n",
    "# ========= 2. 生成模型答案 =========\n",
    "answers_path = out_dir / \"retrieval_answers.json\"\n",
    "if answers_path.exists():\n",
    "    model_answers = json.load(open(answers_path))\n",
    "else:\n",
    "    model_answers = {}\n",
    "\n",
    "for idx, qa in enumerate(tqdm.tqdm(qa_pairs, desc=\"Generating answers\")):\n",
    "    q = qa[\"question\"]\n",
    "    if str(idx) in model_answers:\n",
    "        continue  # 已存在则跳过（断点续跑）\n",
    "\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            api_key=\"sk-5157bc95a7ee4f7e9086a80fd41c69fc\",\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "        # client = OpenAI()\n",
    "        resp = client.chat.completions.create(\n",
    "            model=RESPONDER_MODEL,\n",
    "            # temperature=TEMP,\n",
    "            # max_tokens=512,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an experienced oncologist answering exam-style clinical questions concisely and accurately.\"},\n",
    "                {\"role\": \"user\",    \"content\": q}\n",
    "            ],\n",
    "        )\n",
    "        model_answers[str(idx)] = resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[Responder error @ {idx}] {e}. Retrying in 10 s…\")\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "\n",
    "    # 每 10 条保存一次\n",
    "    if idx % 10 == 9:\n",
    "        json.dump(model_answers, open(answers_path, \"w\",  encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# 最终保存\n",
    "json.dump(model_answers, open(answers_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "# ========= 3. 调用 Judge 评分 =========\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fae8d5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Judging answers: 100%|██████████| 100/100 [03:10<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Summary ===\n",
      "Samples evaluated : 100/100\n",
      "Average score     : 3.27 / 5\n",
      "Score distribution: {0: 21, 1: 5, 2: 15, 3: 1, 4: 1, 5: 57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# out_dir = pathlib.Path(\"../res/deepseek-v3\")\n",
    "scores_path = out_dir / \"retrieval_scores.json\"\n",
    "if scores_path.exists():\n",
    "    judge_scores = json.load(open(scores_path))\n",
    "else:\n",
    "    judge_scores = {}\n",
    "\n",
    "judge_prompt_tpl = \"\"\"You are an impartial medical board examiner.\n",
    "Score the model answer against the reference answer on a 0–5 scale,\n",
    "using the *refined* rubric below.  If unsure between two scores, pick\n",
    "the **lower** one.\n",
    "\n",
    "Rubric:\n",
    "5 = Covers **all** key clinical facts in the reference; any additional\n",
    "    explanations are factually correct *and clinically relevant*; no\n",
    "    inaccuracies, unsafe statements, or major omissions.\n",
    "4 = ≥90 % of key facts correct; extra content is correct; at most one\n",
    "    minor omission **or** wording inaccuracy that does not alter meaning.\n",
    "3 = 70-89 % of key facts covered; may include a few minor errors or\n",
    "    omissions, but no clinically dangerous advice.\n",
    "2 = 40-69 % of key facts **or** ≥1 moderate factual error/omission; some\n",
    "    irrelevant or redundant statements allowed.\n",
    "1 = <40 % of key facts **or** major inaccuracies; content mostly\n",
    "    irrelevant or confusing.\n",
    "0 = Blank, nonsense, or any clearly unsafe recommendation.\n",
    "\n",
    "Penalty rules:\n",
    "• Extra content that is factually correct & relevant → **no penalty**.\n",
    "• Extra but irrelevant OR factually wrong content → lower the score.\n",
    "• Any unsafe or potentially harmful statement → max score = 1.\n",
    "\n",
    "Return exactly one line:\n",
    "\"<score 0-5>: <concise 1–2 sentence justification>\"\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference answer:\n",
    "{ref_answer}\n",
    "\n",
    "Model answer:\n",
    "{model_answer}\n",
    "\"\"\"\n",
    "\n",
    "for idx, qa in enumerate(tqdm.tqdm(qa_pairs, desc=\"Judging answers\")):\n",
    "    if str(idx) in judge_scores:\n",
    "        continue\n",
    "\n",
    "    prompt = judge_prompt_tpl.format(\n",
    "        question=qa[\"question\"],\n",
    "        ref_answer=qa[\"answer\"],\n",
    "        model_answer=model_answers.get(str(idx), \"\")\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-12-01-preview\",\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=JUDGE_MODEL,\n",
    "            temperature=0,\n",
    "            max_tokens=120,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert clinical examiner.\"},\n",
    "                {\"role\": \"user\",    \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        # raw = resp.choices[0].message.content.strip()\n",
    "        # score = raw.split()[0]  # 第一词应该是分数\n",
    "        # judge_scores[str(idx)] = {\"score\": float(score), \"explanation\": raw}\n",
    "\n",
    "        raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "        m = re.search(r\"\\b([0-5](?:\\.\\d+)?)\\b\", raw)\n",
    "        if not m:\n",
    "            print(f\"[Judge format error @ {idx}] {raw}\")\n",
    "            continue                      # 或者 retry\n",
    "\n",
    "        score = float(m.group(1))\n",
    "        judge_scores[str(idx)] = {\"score\": score, \"explanation\": raw}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Judge error @ {idx}] {e}. Retrying in 5 s…\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    if idx % 10 == 9:\n",
    "        json.dump(judge_scores, open(scores_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "json.dump(judge_scores, open(scores_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# ========= 4. 汇总统计 =========\n",
    "scores = [v[\"score\"] for v in judge_scores.values()]\n",
    "avg   = sum(scores) / len(scores)\n",
    "dist  = defaultdict(int)\n",
    "for s in scores:\n",
    "    dist[int(s)] += 1\n",
    "\n",
    "print(\"\\n=== Evaluation Summary ===\")\n",
    "print(f\"Samples evaluated : {len(scores)}/{len(qa_pairs)}\")\n",
    "print(f\"Average score     : {avg:.2f} / 5\")\n",
    "print(\"Score distribution:\", dict(sorted(dist.items())))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
